{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers & Perceptrons\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A soft introduction to Machine Learning (ML): *Supervised Learning***<br>\n",
    "\n",
    "*Example: Movie Recommender Problem*\n",
    "\n",
    "Given a series of movies with ratings ($-1$ or $+1$, *dislike/like* for simplicity), build a feature vector ($X \\in \\mathbb {R}^d$) for each movie:\n",
    "\n",
    "$x^{(1)} = \\begin{bmatrix} 1 & 0 & 1 & 0 & \\dots \\end{bmatrix}^T$ contains information about the characteristics of the movie, where each binary score is a representation of some characteristics about the movie, e.g. *action, romance, spielberg-directed*\n",
    "\n",
    "$y^{(1)} = \\begin{bmatrix} 1 \\end{bmatrix}^T$ where $y \\in [-1, 1]$, is the associated rating to the given movie\n",
    "\n",
    "Given these feature vectors and its associated ratings, the ML model estimates the parameters that would map the feature vectors to its associated ratings. With a new movie comes a new feature vector, where the model uses the estimated parameters to predict the rating for the given movie.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Linear Classifiers**\n",
    "\n",
    "A linear classifier is a function, $h(\\mathbb{X})$, that maps the feature vector, $\\mathbb{X}$, to its associated labels, $\\mathbb{Y}$. This classifier linearly divides space into two with a hyperplane where training data lies. Given a point $x$ in the space, the classifier $h(x)$ outputs a label, depending on where the point $x$ exists in a multidimensional space among the two linearly divided spaces.\n",
    "\n",
    "To evaluate how well a linear classifier performs, we compute a training error, $E_i (h) = h(x^{(i)}) \\neq y^{(i)}$ and produces $1$ if there is an error, or $0$ otherwise. The fraction of training errors is $\\therefore E_n(h) = \\frac{1}{n} \\sum_{i=1}^{n} E_i (h)$\n",
    "\n",
    "A set of classifiers, $H \\in \\mathbb{H}$, lives in a hypothesis space which contains all possible classifiers for the given task.\n",
    "\n",
    "A perceptron algorithm, $\\hat h = \\mathbb{A} (S_n, \\mathbb{H})$, where $S_n$ is the training data, returns a classifier ($\\hat h$) from the hypothesis space ($\\mathbb{H}$) that best fits the training data.\n",
    "\n",
    "The mathematical understanding of linear classifiers is as follows:\n",
    "\n",
    "A linear hyperplane (*decision boundary*) can be as simple as follows, $X: \\theta_1 X_1 + \\theta_2 X_2 = 0$ or it can also be expressed in matrix form, $\\theta^T X = 0$, where $\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$ and $X = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}$. From here, we can observe that $\\theta$ is orthogonal to $X$ for this hyperplane that cuts through the origin.\n",
    "\n",
    "For a linear classifier that cuts through the *origin* would simply be expressed this way, $h(X; \\theta) = \\mathbb{1} (\\theta^T X > 0)$, where $\\theta \\in \\mathbb{R}^d$\n",
    "\n",
    "To generalize beyond a classifier beyond the origin, this is simply expressed with an additional term, $\\theta_0$, that shifts the hyperplane from the origin, where $\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$ and $X = \\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\end{bmatrix}$, such that $\\theta^T X = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 = 0$ and $\\theta$ continues to be orthogonal to $X$\n",
    "\n",
    "<img alt=\"Linear Classifier\" src=\"assets/linear_classifier.png\" width=\"400\">\n",
    "\n",
    "A full definition of linear classifiers is based on a clean seperation of the observations and it follows:\n",
    "\n",
    "Training examples $S_n = \\{(x^{(i)}, y^{(i)}), i = 1, \\dots, n\\}$ are *linearly seperable* if there exists a parameter vector $\\hat \\theta$ and offset parameter $\\hat \\theta_0$ such that $y^{(i)} \\cdot (\\hat \\theta \\cdot x^{(i)} + \\hat \\theta_0) > 0$ for all $i = 1, \\dots, n$\n",
    "\n",
    "Given that $y^{(i)}$ is part of the equation, then $0$ is not a possible value and should not be used as a label.\n",
    "\n",
    "****\n",
    "\n",
    "**The Perceptron Algorithm**\n",
    "\n",
    "$E_n (\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1} (y^{(i)} \\cdot (\\theta x^{(i)} + \\theta_0) \\leq 0)$\n",
    "\n",
    "Procedure:\n",
    "1. Start with a zero parameter, $\\theta = 0$ vector\n",
    "2. for $i = 1, \\dots, n$, do:\n",
    "    - If label and prediction is wrong, $y^{(i)}(\\theta \\cdot x^{(i)}) \\leq 0$\n",
    "    - Then update the parameter, $\\theta = \\theta + y^{(i)}x^{(i)}$\n",
    "    - Repeat the procedure for $T$ times across the training set again until no further parameter update is necessary\n",
    "    - Every update always minimizes the training error\n",
    "        - $y^{(i)} ((\\theta + y^{(i)}x^{(i)}) \\cdot x^{(i)} + \\theta_0 + y^{(i)}) - y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0) > 0$\n",
    "        - The formula above simplifies to $(y^{(i)})^2 \\cdot (\\lVert x^{(i)} \\rVert^2 + 1) > 0$\n",
    "    \n",
    "    \n",
    "3. For a sufficiently large $T$ then it converges to a parameter\n",
    "\n",
    "<img alt=\"Perceptron\" src=\"assets/perceptron.png\" width=\"400\">\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
