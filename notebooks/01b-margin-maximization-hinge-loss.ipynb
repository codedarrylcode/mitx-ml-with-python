{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinge loss, Margin boundaries, and Regularization\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Large Margin Classifier (SVM)**<br>\n",
    "\n",
    "Consider a line $L \\in \\mathbb{R}^2$\n",
    "\n",
    "$L: \\theta \\cdot x + \\theta_0 = 0$\n",
    "\n",
    "where $\\theta$ is a vector orthogonal to the line $L$\n",
    "\n",
    "The **decision boundary** is the set of points $x$ which satisfies the equation $L$ and is equi-distance or between the **margin boundaries** (*support vectors*) which is the set of points $x$ which satisfy $\\theta \\cdot x + \\theta_0 = \\pm 1$\n",
    "\n",
    "The distance between the decision boundary and either margin boundaries / support vectors is equi-distance and its distance can be computed with $\\frac{1}{\\Vert \\theta \\Vert}$\n",
    "\n",
    "$\\gamma_i (\\theta, \\theta_0) = \\frac{y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0)}{\\Vert \\theta \\Vert}$, such that for a correctly classified point on the margin boundary, the distance will be equal to $\\frac{1}{\\Vert \\theta \\Vert}$ - to compute the shortest distance between a point and the decision boundary, $\\frac{\\vert (\\theta \\cdot x + \\theta_0) \\vert}{\\Vert \\theta \\Vert^2}$\n",
    "\n",
    "To optimize the distance between decision and margin boundaries, maximize $\\frac{1}{\\Vert \\theta \\Vert}$\n",
    "\n",
    "<img alt=\"Large Margin Classifier\" src=\"assets/large_margin_classifier.png\" width=\"400\">\n",
    "\n",
    "****\n",
    "\n",
    "**Large margin and classification accuracy as optimization**\n",
    "\n",
    "- Hinge Loss\n",
    "\n",
    "    If classifier and actual class agrees, then $y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0) \\geq 1$ (which we will define as $Z$). We can then define Hinge loss to equal $0$ if $Z \\geq 1$, else $1 - Z$ if $Z < 1$\n",
    "\n",
    "    $Loss_h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0))$\n",
    "    \n",
    "\n",
    "- Regularization: *towards max margin between decision boundaries and support vectors*\n",
    "    \n",
    "    $\\max \\frac{1}{\\Vert \\theta \\Vert}$ is the same as trying to minimize $\\frac{1}{2} \\Vert \\theta \\Vert^2$\n",
    "\n",
    "\n",
    "- Objective function\n",
    "\n",
    "    The parameter choice of $\\theta$, $\\theta_0$ is balanced between hinge loss (*ensure points are correctly classified*) and regularization (*maximizes distance between decision and margin boundaries*).\n",
    "\n",
    "    $J(\\theta, \\theta_0) = \\frac{1}{n} \\sum_{i = 1}^{n} Loss_h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0)) + \\frac{\\lambda}{2} \\Vert \\theta \\Vert^2$\n",
    "    \n",
    "    where $\\lambda$ is the regularization parameter which weighs how much we favour large margin versus accuracy of the classification, i.e. larger $\\lambda$ favours large margin and $\\lambda > 0$\n",
    "    \n",
    "****\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "The objective function with stochastic gradient descent can be rewritten as follows:\n",
    "\n",
    "$J(\\theta, \\theta_0) = \\frac{1}{n} \\sum_{i = 1}^{n} [Loss_h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0)) + \\frac{\\lambda}{2} \\Vert \\theta \\Vert^2 ]$\n",
    "\n",
    "With stochastic gradient descent, we choose $i \\in \\{1, \\dots, n\\}$ at random, and update $\\theta$ such that:\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\eta \\nabla _{\\theta } \\big [\\text {Loss}_ h(y^{(i)}(\\theta \\cdot x^{(i)} + \\theta _0) ) + \\frac{\\lambda }{2}\\mid \\mid \\theta \\mid \\mid ^2 \\big]$\n",
    "\n",
    "where $\\eta$ is the step size and $\\nabla_{\\theta}$ is the derivative of the objective function with respect to $\\theta$\n",
    "\n",
    "Suppose $Loss_h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0) > 0$ then:\n",
    "\n",
    "$Loss_h (y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0) = 1 - y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0)$\n",
    "\n",
    "and $\\therefore \\nabla_{\\theta} = - y^{(i)} x^{(i)}$\n",
    "\n",
    "\n",
    "In SGD, $\\theta$ is updated even when there is no mistake, i.e. $Loss_h = 0$ for the given point with $\\theta - \\eta \\lambda \\theta$ or $(1 - \\lambda \\eta) \\theta$, but updates with $(1 - \\lambda \\eta) \\theta + \\eta y^{(i)} x^{(i)}$ if there is some loss.\n",
    "\n",
    "This update when there is no mistake, helps to maximize the margin between decision/margin boundaries such that some training examples will be on the margin boundary itself.\n",
    "\n",
    "This means that if we remove all points that are support vectors, we will get a different $\\theta$, $\\theta_0$ and on the reverse, removing a point that is not a support vector will not return a different set of parameters\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
