{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Objective: *Minimize Empirical Risk***\n",
    "\n",
    "$R_n(\\theta)$ where $n$ is the number of samples and $\\theta$ is the vector parameter, such that the empirical risk can be defined as follows:\n",
    "\n",
    "$R_n (\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y^{(i)} - \\theta x^{(i)})^2 / 2$\n",
    "\n",
    "But in practice, the idea is to minimize the empirical risk when generalized outside of the training set.\n",
    "\n",
    "There are two basic mistakes when it comes to the application of linear regression:\n",
    "\n",
    "1. **Structural**: Where the relationship between the dependent and independent variables are non-linear. Consider a much broader family of non-linear functions as the estimator\n",
    "\n",
    "\n",
    "2. **Estimation**: Very limited training data which doesn't generalize well to the population\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Algorithm 1: *Gradient Descent***\n",
    "\n",
    "Derivative of the empirical risk function:\n",
    "\n",
    "$\\nabla_{\\theta} (y^{(t)} - \\theta x^{(t)})^2 / 2 = - (y^{(t)} - \\theta x^{(t)}) \\cdot x^{(t)}$\n",
    "\n",
    "Process: \n",
    "\n",
    "- Initialize $\\theta = 0$\n",
    "- Randomly pick $t = \\{ 1, \\dots, n \\}$\n",
    "- Update $\\theta := \\theta - \\eta \\cdot [- (y^{(t)} - \\theta x^{(t)}) \\cdot x^{(t)}]$, where $\\eta$ is the learning rate parameter and could be set to $\\eta_k = \\frac{1}{1+k}$, where $k$ is the number of iterations and the algorithm corrects with smaller steps as algorithm has gone on for many steps\n",
    "\n",
    "****\n",
    "\n",
    "**Closed form solution: *Linear Algebra***\n",
    "\n",
    "$\\hat \\theta = (X^T X)^{-1} X^T Y$\n",
    "\n",
    "Necessary conditions:\n",
    "\n",
    "- Training samples, $n$, must be bigger than dimensionality, $d$\n",
    "- The matrix, $X^T X$, must be invertible, $\\therefore X$ must be invertible\n",
    "\n",
    "****\n",
    "\n",
    "**Generalization & Regularization**\n",
    "\n",
    "Regularization helps to prevent the estimated parameters to overfit the training data and improve the model's ability to generalize beyond the training set, by adding a new parameter in the loss function, $\\frac{\\lambda}{2} \\Vert \\theta \\Vert^2$. Intuitively, it does so by pushing $theta$ to 0, unless there is a strong pattern to justify otherwise.\n",
    "\n",
    "\n",
    "Regularization example, *Ridge Regression*\n",
    "\n",
    "Loss function with regularization,\n",
    "\n",
    "$J_{\\lambda, n} (\\theta) = \\frac{\\lambda}{2} \\Vert \\theta \\Vert^2 + R_n (\\theta)$\n",
    "\n",
    "Derivative of loss function with regularization\n",
    "\n",
    "$\\nabla_{\\theta} J_{\\lambda, n} (\\theta) = \\lambda \\theta - (y^{(t)} - \\theta x^{(t)}) \\cdot x^{(t)}$\n",
    "\n",
    "then, we update $\\theta$\n",
    "\n",
    "$\\theta := \\theta - \\eta \\cdot [\\lambda \\theta - (y^{(t)} - \\theta x^{(t)}) \\cdot x^{(t)}] = (1 - \\eta \\lambda) \\cdot \\theta - \\eta \\cdot [- (y^{(t)} - \\theta x^{(t)}) \\cdot x^{(t)}]$\n",
    "\n",
    "The second part of the update is the same, but the first part has changed from $\\theta$ to $(1 - \\eta \\lambda) \\cdot \\theta$, where every learning step tries to push $\\theta$ down to zero\n",
    "\n",
    "Whenever $\\lambda$ increases, we care to push $\\theta$ as close to zero as possible which increases ability to generalize better. When $\\lambda$ is zero, then there is no regularization and may cause overfitting and not generalize well.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
