{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear classification\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature transformation for non-linearly seperable spaces**<br>\n",
    "\n",
    "We can adapt linear classifiers by mapping all examples in $x \\in \\mathbb{R}^d$ to a higher dimensional space with $\\phi(x) \\in \\mathbb{R}^p$, where $p > d$, such that the training examples will now be linearly seperable in the higher dimensional space.\n",
    "\n",
    "This mapping, $\\phi(x)$, can be done in several ways. One way is to add polynomial terms of the original $d$ dimensions, another is to use interactions between the dimensions.\n",
    "\n",
    "For example, given $X \\in \\mathbb{R}^2$\n",
    "\n",
    "$X = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\in \\mathbb{R}^2$\n",
    "\n",
    "$\\phi(X) = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_1^2 \\\\ X_2^2 \\\\ X_1 X_2 \\end{bmatrix} \\in \\mathbb{R}^5$\n",
    "\n",
    "<img alt=\"Feature Transformation\" src=\"assets/non_linear_classification.png\" width=\"800\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Non-linear Regression**\n",
    "\n",
    "This feature transformation could easily be applied to transform linear regression to non-linear regression as well. \n",
    "\n",
    "To prevent overfitting by adding more polynomial terms, which will fit the training examples perfectly but fails to generalize well, use cross-validation (*leave-one-out, or k-fold*) to find $\\phi(x)$ such that it minimizes the validation error.\n",
    "\n",
    "<img alt=\"Adding Polynomial Terms to Linear Regression\" src=\"assets/non_linear_regression.png\" width=\"400\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Kernels for computational efficiency**\n",
    "\n",
    "Applying polynomial features in classifiers/regressions can result in a very high dimensional space which may be computationally expensive. For example, for $X \\in \\mathbb{R}^d$ then a polynomial order of 3 will result in $d + \\binom{d+2-1}{2} + \\binom{d+3-1}{3}$ dimensions.\n",
    "\n",
    "*Perceptron algorithm*:\n",
    "\n",
    "- Set $\\theta = 0$\n",
    "- Run through $i = 1, \\dots, n$\n",
    "- if $y^{(i)} \\theta \\cdot \\phi(x^{(i)}) \\leq 0$, then update $\\theta \\leftarrow \\theta + y^{(i)} \\cdot \\phi(x^{(i)})$\n",
    "\n",
    "When expressed differently, we can see that the update to $\\theta$ is the sum of mistaken classification multiplied by $y^{(i)} \\cdot \\phi(x^{(i)})$, since $\\theta$ starts at 0\n",
    "\n",
    "$\\theta = \\sum_{j = 1}^{n} \\alpha_j \\cdot y^{(j)} \\cdot \\phi(x^{(j)})$, where $\\alpha_j$ is an indicator of mistaken classification\n",
    "\n",
    "If we multiply both sides by $\\phi(x^{(i)})$, then\n",
    "\n",
    "$\\theta \\cdot \\phi(x^{(i)}) = \\sum_{j = 1}^{n} \\alpha_j \\cdot y^{(j)} \\cdot \\phi(x^{(j)}) \\cdot \\phi(x^{(i)})$\n",
    "\n",
    "where $\\phi(x^{(j)}) \\cdot \\phi(x^{(i)})$ is the kernel function, $k(x^{(j)}, x^{(i)})$\n",
    "\n",
    "$\\therefore$ Instead of running the 3rd step using $y^{(i)} \\theta \\cdot \\phi(x^{(i)}) \\leq 0$, we can express this using the kernel function, $y^{(i)} \\cdot \\sum_{j = 1}^{n} \\alpha_j \\cdot y^{(j)} \\cdot k(x^{(j)}, x^{(i)})$ and update $\\alpha_i \\leftarrow \\alpha_i + 1$\n",
    "\n",
    "*Kernel Perceptron algorithm*:\n",
    "\n",
    "- Initialize $a_1, a_2, \\dots, a_n = 0$\n",
    "- For $t = 1, \\dots, T$\n",
    "- For $i = 1, \\dots, n$\n",
    "- if $\\left(\\text {Mistake Condition Expressed in }\\,  \\alpha _ j\\right)$, update $a_j$ appropriately\n",
    "\n",
    "****\n",
    "\n",
    "**Feature engineering with kernels**\n",
    "\n",
    "Composition rules of kernels:\n",
    "\n",
    "1. $K(x, x') = 1$ is a kernel function\n",
    "2. Let $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and $K(x, x')$ is a kernel. Then so is $\\tilde K(x, x') = f(x) K(x, x') f(x')$ or $\\tilde \\phi(x) = f(x) \\cdot \\phi(x)$\n",
    "3. $K(x, x') = K_1(x, x') + K_2(x, x')$ is also a kernel, given $K_1, K_2$ are kernels\n",
    "4. $K(x, x') = K_1(x, x') \\cdot K_2(x, x')$ is also a kernel, given $K_1, K_2$ are kernels\n",
    "\n",
    "****\n",
    "\n",
    "**Kernels and classifiers**\n",
    "\n",
    "*Radial Basis Function (RBF) Kernel*\n",
    "\n",
    "$K(x, x') = \\exp (-\\frac{1}{2} \\Vert x - x' \\Vert^2)$\n",
    "\n",
    "*Random forest classifier*\n",
    "\n",
    "Procedure:\n",
    "- Bootstrap sample with replacement\n",
    "- Build a (randomized) decision tree\n",
    "- Repeat N iterations\n",
    "- Average N predictions (ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
