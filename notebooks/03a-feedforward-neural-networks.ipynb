{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gentle introdution to Neural Networks**<br>\n",
    "\n",
    "A basic neural network with one *input* layer and an *output* layer can be represented pictorially as follows\n",
    "\n",
    "<img alt=\"Basic Neural Network\" src=\"assets/basic_neural_network.png\" width=\"300\">\n",
    "\n",
    "Then, the neural network computes a non-linear weighted combination of its input:\n",
    "\n",
    "$\\hat y = f(z)$, where $z = \\omega_0 + \\sum_{i=1}^{d} x_i \\omega_i$ and $f$ is generally a non-linear function called the activation function.\n",
    "\n",
    "Common activation functions include:\n",
    "\n",
    "- Rectified linear function (ReLU)\n",
    "\n",
    "    $f(z) = \\max \\{0, z\\}$ which forces all negative values to flatten out to 0\n",
    "\n",
    "\n",
    "- Hyperbolic tangent function (tanh)\n",
    "\n",
    "    $f(z) = \\tanh(z) = \\frac{\\exp^{z} - \\exp^{-z}}{\\exp^z + \\exp^{-z}} = 1 - \\frac{2}{\\exp^{2z} + 1}$\n",
    "    \n",
    "    \n",
    "\n",
    "Eseentially, a neural network architecture is represented by:\n",
    "\n",
    "- Functions, $f$\n",
    "- Features, $X$ as an input layer\n",
    "- Weights, $W$\n",
    "\n",
    "<img alt=\"Neural Network Architecture\" src=\"assets/neural_network_architecture.png\" width=\"500\">\n",
    "\n",
    "****\n",
    "\n",
    "**Introduction to Deep (*Feedforward*) Neural Networks**\n",
    "\n",
    "The depth of a neural network is represented by the number of transformations (*layers*) from input (*exclusive*) to output layer (*inclusive*), which makes a *deep* neural network. The width of the network is the maximum number of nodes in a layer.\n",
    "\n",
    "A deep neural network can mimic the human's brain network in breaking down information and processing it to arrive at a final output, say for e.g. given an image, broken into pixel data and finally output a binary classification result.\n",
    "\n",
    "Suppose a 2D representation of data that is not linearly seperable, we can use a hidden neural network to transform its features to become linearly seperable and easily solvable. Here is the transformation using *linear*, *tanh* and *ReLU* activation functions.\n",
    "\n",
    "<img alt=\"Linear Activation\" src=\"assets/feature_transformation_neural_networks.png\" width=\"400\">\n",
    "\n",
    "<img alt=\"Tanh Activation\" src=\"assets/feature_transformation_tanh.png\" width=\"400\">\n",
    "\n",
    "<img alt=\"ReLU Activation\" src=\"assets/feature_transformation_relu.png\" width=\"400\">\n",
    "\n",
    "****\n",
    "\n",
    "**Backpropagation Algorithm**\n",
    "\n",
    "In evaluating the neural network, typically we compute the model's loss against the true labels. A loss function is defined and can be represented as $Loss(y, f_L)$, where $f_L$ is the final output from the neural network and can be any loss function.\n",
    "\n",
    "Then, we apply the idea of stochastic gradient descent to optimize the weights against the loss function by taking the derivative of the loss against each weight, $w_{ij}$, where $i$ represents the layer index and $j$ represents the node index.\n",
    "\n",
    "For example, in a single node, multiple layers network, we can represent the derative this way.\n",
    "\n",
    "<img alt=\"Optimizing weights\" src=\"assets/optimizing_weights.png\" width=\"400\">\n",
    "\n",
    "To find the derivative of the loss with respect to $f_1$, we can do the following:\n",
    "\n",
    "<img alt=\"Derivative of loss with respect to first function\" src=\"assets/backpropagation_one_step.png\" width=\"400\">\n",
    "\n",
    "In general, we can find the derivative of the loss with respect to the function at any of the layers with the following:\n",
    "\n",
    "<img alt=\"Loss Derivative against function\" src=\"assets/loss_derivative_respect_to_function.png\" width=\"200\">\n",
    "\n",
    "We can then start from the last layer and *propagate* backwards and use a closed-form formula to find the derivative of the loss against the first function.\n",
    "\n",
    "<img alt=\"Derivative of loss at first function\" src=\"assets/loss_derivative_at_first_function.png\" width=\"500\">\n",
    "\n",
    "Then finally, we can compute the derivative using the first formula and apply stochastic gradient descent to find the optimal $w_1$\n",
    "\n",
    "****\n",
    "\n",
    "**Overly-complex models generate good performance**\n",
    "\n",
    "- Overcapacity: adding random units to the neural network architecture helps to improve model performance\n",
    "    - As long as most random units are doing something meaningful, then it will improve model performance\n",
    "    - A harder task may not be linearly seperable with two units and may require more random units\n",
    "    - $\\therefore$ Larger models tend to to be easier to learn as units are adjusted collectively and sufficiently to solve the task\n",
    "\n",
    "\n",
    "<img alt=\"Adding random units\" src=\"assets/adding_random_units.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- Initialization plays a role in finding a good solution\n",
    "    - Use random offset initialization to avoid symmetries caused by concentration around zero\n",
    "\n",
    "<img alt=\"Random Initialization\" src=\"assets/random_offset_initialization.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
