{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling sequences/temporal problems**<br>\n",
    "\n",
    "Common problems:\n",
    "\n",
    "- Forward predictions in time-series data\n",
    "- Language modeling: what word comes next given a sentence?\n",
    "\n",
    "Some common challenges are for instance, how many steps back should we look at, how do we retain important, meaningful temporal positions that are far back? This presents challenges to engineer a mapping of *history* into a vector representation. In general, RNNs automatically addresses some of these issues that needs to be engineered with sequences.\n",
    "\n",
    "****\n",
    "\n",
    "**Encoding with RNN**\n",
    "\n",
    "*We can encode everything - images, sentences, words, even events... and represent it in a new way using vectors*\n",
    "\n",
    "$s_t = \\tanh ( W^{s, s} s_{t-1} + W^{s, x} x_t )$\n",
    "\n",
    "where $W^{s, s}, W^{s, x}$ are the weight parameters that would transform the previous state, $s_{t-1}$, along with new information, $x_t$, into a new vector that feeds into the activation function to produce a new state, $s_t$, at time $t$\n",
    "\n",
    "<img alt=\"RNN Encoding\" src=\"assets/rnn_transformation.png\" width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "For a given sentence, we can start with a zero-vector state and add a new word in for each new state and continue for the rest of the sentence and then output a vector that represents the sentence. \n",
    "\n",
    "Each new word would represent a new layer in this network and critically each layer shares the same parameters across the network and applies the same transformation to produce the next state.\n",
    "\n",
    "<img alt=\"RNN Encoding\" src=\"assets/sentence_transformation.png\" width=\"500\">\n",
    "\n",
    "****\n",
    "\n",
    "**Gating and LSTM**\n",
    "\n",
    "Now, we introduce the idea of gating. \n",
    "\n",
    "A gate vector, $g_t$ of the same dimension as $s_t$, determines *how much information to overwrite in the next state*.\n",
    "\n",
    "$g_t = sigmoid(W^{g, s} s_{t-1} + W^{g, x} x_t)$\n",
    "\n",
    "where $g_t \\in \\{0, 1\\}$ and has the same dimensions as $s_t$. This is then applied in the computation of $s_t$ below:\n",
    "\n",
    "$s_t = (1 - g_t) \\bigodot s_{t-1} + g_t \\bigodot \\tanh (W^{s,s} s_{t-1} + W^{s, x} x_t)$\n",
    "\n",
    "where $\\bigodot$ represents element-wise multiplication between the two vectors\n",
    "\n",
    "This essentially means that $g_t$ controls the amount of information that we want to overwrite from the new output, or similarly, how much of the previous state we would like to retain$\n",
    "\n",
    "Using *LSTM*, we can add model complexity by applying multiple gates to control how information is read from the previous state and produced into the next state:\n",
    "\n",
    "<img alt=\"LSTM Gating\" src=\"assets/lstm.jpg\" width=\"500\">\n",
    "\n",
    "****\n",
    "\n",
    "**Title**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
