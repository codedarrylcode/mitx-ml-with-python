{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification vs Clustering**<br>\n",
    "\n",
    "In classification, we have features and labels where $S_n = \\{ (x^{(i)}, y^{(i)}) | i = 1, \\dots, n \\}$\n",
    "\n",
    "While in clustering, we only have features with no associated labels where $S_n = \\{ (x^{(i)}) | i = 1, \\dots, n \\}$ and the key objective is to **find some meaningful structure/patterns within the dataset** and yet we need to define what is our new cost function to optimize, without associated labels.\n",
    "\n",
    "Interesting examples of clustering:\n",
    "\n",
    "- Google news clustering of similar news from different sites into one headline\n",
    "- Image quantization, each image has $1024 \\times 1024$ pixels $\\times$ $24$ RGB bits $\\approx 3 \\times 10^6$, or we can compress this further by using only 32 colours, represented by $5$ RGB bits ($2^5$), where each RGB bit can be a cluster to compress similar colors together\n",
    "\n",
    "To think about clustering more intuitively, we can think about it as partitioning. \n",
    "\n",
    "Given an input dataset, $S_n = \\{ (x^{(i)}) | i = 1, \\dots, n \\}$, and the number of desired/target clusters, $K$, clustering will generate an output of $C_1, \\dots, C_K$ partitions/clusters, such that: \n",
    "\n",
    "- The union of all partitions will represent $S_n$, i.e. every element is included in one and only one of the subsets\n",
    "- The intersection of any two partitions will be empty, $C_i \\bigcap C_j = \\emptyset$ for any $i \\neq j$ in $\\{ 1, \\dots, k \\}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Similarity measures - cost functions for clustering**\n",
    "\n",
    "To evaluate how well a clustering algorithm performs, we'll need to define a cost function to optimize, such that:\n",
    "\n",
    "$\\text{cost} (C_1, \\dots, C_K) = \\sum_{j = 1}^{K} \\text{cost}(C_j)$\n",
    "\n",
    "This is because given a dataset, $S_n = \\{ (x^{(i)}) | i = 1, \\dots, n \\}$ and $K = 2$, there are multiple ways to form the $K$ clusters as shown below. How do we evaluate which cluster is a better fit?\n",
    "\n",
    "<img alt=\"Two different clusterings\" src=\"assets/two_different_clusterings.png\" width=\"500\">\n",
    "\n",
    "Some possible options as distance measures to compare similarity between two vectors:\n",
    "\n",
    "- Diameter of a cluster\n",
    "- Mean distance of all points within a cluster\n",
    "- Other distance metrics\n",
    "    - Cosine similarity, $\\cos (x^{(i)}, x^{(j)}) = \\frac{x^{(i)} \\cdot x^{(j)}}{\\Vert x^{(i)} \\Vert \\cdot \\Vert x^{(j)} \\Vert}$\n",
    "    - Euclidean, $\\text{dist} (x^{(i)}, x^{(j)}) = \\Vert x^{(i)} - x^{(j)} \\Vert^2$\n",
    "\n",
    "Using the representative of a cluster, $z$, we can evaluate the cost such that:\n",
    "\n",
    "$\\text{cost} (C_j) = \\sum_{i \\in C_j} \\text{dist} (x^{(i)}, z)$\n",
    "\n",
    "Using the Euclidean distance as a way to define the cost:\n",
    "\n",
    "$\\text{cost} (C_j) = \\sum_{i \\in C_j} \\Vert x^{(i)} - z^{(j)} \\Vert^2$\n",
    "\n",
    "****\n",
    "\n",
    "**K-Means Algorithm**\n",
    "\n",
    "1. Randomly select $z^{(1)}, \\dots, z^{(K)}$, where $z^{(i)}$ are representatives of $K$ clusters\n",
    "2. Iterate and update the representatives until *covergence*, i.e. no change in cost function\n",
    "    1. Given $z^{(1)}, \\dots, z^{(K)}$, assign each $x^{(i)}$ to the closest $z_j$ such that: $\\text{cost} (z^{(i)}, \\dots, z^{(k)}) = \\sum_{i = 1}^{n} \\min_{j = 1, \\dots, K} \\Vert x^{(i)} - z^{(j)} \\Vert^2$\n",
    "        \n",
    "    2. Given $C_1, \\dots, C_K$, find the best representatives $z_1, \\dots, z_k$, such that: $z_j = \\text{argmin}_z \\sum_{i \\in C_j} \\Vert x^{(i)} - z \\Vert^2$, such that $z_j$ is the centroid of the new assigned cluster\n",
    "    \n",
    "    Proof of 'best' representative being the cluster centroid:\n",
    "    \n",
    "    First, compute the gradient of the cost function\n",
    "    \n",
    "    $\\nabla _{z_ j}\\left(\\sum _{i \\in \\mathbb {C}_ j} \\| x^{(i)} - z_ j\\| ^2\\right) = 0$\n",
    "    \n",
    "    $\\nabla _{z_ j} = \\displaystyle \\sum _{i \\in \\mathbb {C}_ j} -2(x^{(i)} - z_ j) = 0$\n",
    "    \n",
    "    $\\therefore z_ j = \\frac{\\sum _{i \\in C_ j} x^{(i)}}{|C_ j|}$\n",
    "    \n",
    "    where ${|C_ j|}$ is the size of the cluster and $z_j$ is the center of mass or centroid of the $j$th cluster\n",
    "    \n",
    "The K-means algorithm is not deterministic, i.e. poor initialization can lead to local *convergence*. To resolve this, we can run K-means for a reasonably large number of iterations and pick the clustering which has the lowest cost. The general K-means algorithm will only work with the Euclidean distance due to the update procedure and the proof above.\n",
    "\n",
    "Disadvantages / Limitations:\n",
    "\n",
    "- Manual choice of $K$\n",
    "- Not robust to outliers as centroids can be dragged around by outliers or outliers might get their own cluster\n",
    "- Does not scale well with increasing number of dimensions\n",
    "\n",
    "****\n",
    "\n",
    "**K-Medoids**\n",
    "\n",
    "1. Randomly initialize, $\\{ z^{(i)}, \\dots, z^{(K)} \\} \\in \\{ x^{(i)}, \\dots, x^{(n)} \\}$\n",
    "2. Iterate until there is no change in cost, i.e. *convergence*\n",
    "    1. Assign each point to the closest $z^{(j)}$\n",
    "    2. Select a new point as the representative of the cluster by minimizing the distance between all points and the representative, such that $z^{(j)} \\in \\{ x^{(1)}, \\dots, x^{(n)} \\}$ and $\\sum_{x^{(i)} \\in C_j} \\text{dist} (x^{(i)}, z_j)$\n",
    "    \n",
    "This will always guarantee that the $K$ representatives, $z_1, \\dots, z_K \\in \\{ x_1, \\dots, x_n \\}$ and can be applied with any distance measure as it no longer has to satisfy the requirements of computing the gradient of the cost function.\n",
    "\n",
    "The K-medoids in one iteration has a higher computational complexity, $\\mathcal{O}(n^2 d K)$, as compared to the K-means algorithm, $\\mathcal{O}(ndK)$\n",
    "\n",
    "****\n",
    "\n",
    "**Choosing $K$**\n",
    "\n",
    "One way to pick $K$ is to compute the cost function for different numbers of $K$ such that the optimal choice will be the inflection point where the second derivative is close to zero.\n",
    "\n",
    "<img alt=\"Scree Plot\" src=\"assets/scree_plot.png\" width=\"500\">\n",
    "\n",
    "Another way to consider $K$ is to evaluate how the choice of $K$ affects downstream metrics in a supervised learning problem.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 2.0 5.0\n"
     ]
    }
   ],
   "source": [
    "# Given feature vectors and cluster representatives...\n",
    "import numpy as np\n",
    "X = np.array([\n",
    "    [-1, 2], \n",
    "    [-2, 1],\n",
    "    [-1, 0],\n",
    "    [2, 1],\n",
    "    [3, 2]\n",
    "])\n",
    "X1 = np.copy(X[:3])\n",
    "X2 = np.copy(X[3:])\n",
    "Z  = np.array([[-1, 1], [2, 2]])\n",
    "Z1 = np.copy(Z[0])\n",
    "Z2 = np.copy(Z[1])\n",
    "\n",
    "# Compute the cost using euclidean distance\n",
    "C1 = sum(np.linalg.norm(X1 - Z1, axis = 1))\n",
    "C2 = sum(np.linalg.norm(X2 - Z2, axis = 1))\n",
    "\n",
    "print(C1, C2, C1+C2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
