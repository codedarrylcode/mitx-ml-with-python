{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mixture Models and Expectation-Maximization\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**A gentle introduction to Expectation-Maximization (EM) Algorithm**<br>\n",
    "\n",
    "While a \"hard\" clustering algorithm like k-means or k-medoids can only provide a cluster ID for each data point, the *EM* algorithm, along with the model driving its equations, can provide the posterior probability (\"soft\" assignments) that every data point belongs to any cluster.\n",
    "\n",
    "For a given data point, the probabilistic estimate of it belonging to any cluster can be represented as a multinomial:\n",
    "\n",
    "$x \\sim P(x | \\mu_j, \\sigma_{j}^2)$ \n",
    "\n",
    "where \n",
    "\n",
    "- $j = 1, \\dots, K$\n",
    "- $K$ is the number of clusters\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Gaussian Mixture Model (GMM)**\n",
    "\n",
    "This is a generative model for data $x \\in \\mathbb{R}^d$ and is defined by the following set of parameters:\n",
    "\n",
    "1. $K$, the number of mixture components\n",
    "2. $d$-dimensional Gaussian $\\mathcal{N} (\\mu_j, \\sigma_j^2)$ for every $j = 1, \\dots, K$\n",
    "3. $p_1, \\dots, p_K$, a set of mixture weights which sums to 1, where $\\hat{p_j} = \\frac{\\hat{n_j}}{n}$ and $\\hat{n_j}$ is the number of data points in the cluster (multinomial MLE)\n",
    "\n",
    "The parameters can be collectively represented as:\n",
    "\n",
    "$\\theta = \\{ p_1, \\dots, p_K, \\mu_1, \\dots, \\mu_K, \\sigma_1^2, \\dots, \\sigma_K^2 \\}$\n",
    "\n",
    "The likelihood of observing a point $x$ across all clusters in a GMM is given as:\n",
    "\n",
    "$p(x | \\theta) = \\sum_{j=1}^{K} p_j \\mathcal{N} (x, \\mu_j, \\sigma_j^2)$\n",
    "\n",
    "Given a set of points, $S_n$, we maximize the likelihood of $\\theta$ by finding the likelihood of a point across all clusters and taking the joint probability of all points:\n",
    "\n",
    "$p(S_n | \\theta) = \\prod_{i=1}^{n} \\sum_{j=1}^{K} p_j \\mathcal{N} (x; \\mu_j, \\sigma_j^2)$\n",
    "\n",
    "There is no closed-form solution to finding the parameter set $\\theta$ that maximizes the likelihood and therefore the EM algorithm is necessary.\n",
    "\n",
    "****\n",
    "\n",
    "**Deriving the EM Algorithm**\n",
    "\n",
    "The high-level idea follows two steps:\n",
    "\n",
    "1. Compute the probabilistic soft assignment of points to different clusters given a random initialization (*or k-means*) of the set of parameters characterizing the GMM\n",
    "2. Then compute the likelihood of observing this set of points given the probabilistic weights under the random set of parameters\n",
    "\n",
    "... and we continue these two steps until the parameters converges to the maximum likelihood.\n",
    "\n",
    "<img alt=\"EM Iterations\" src=\"assets/em_iterations.png\" width=\"500\">\n",
    "\n",
    "Formally, these are the steps to EM:\n",
    "\n",
    "1. Randomly initialize the parameters, $\\theta = \\{p_1, \\dots, p_K, \\mu_1, \\dots, \\mu_K, \\sigma_1^2, \\dots, \\sigma_K^2 \\}$\n",
    "\n",
    "2. **E-step**: Finds the posterior probability that a certain point $x_i$ belonging to a cluster $j$\n",
    "\n",
    "    $p(j | i) = \\frac{p_j \\mathcal{N}(x^{(i)}; \\mu_j, \\sigma_j^2 \\cdot I)}{p(x|\\theta)}$\n",
    "\n",
    "\n",
    "3. **M-step**: Re-estimate the parameters by maximizing the likelihood\n",
    "\n",
    "    $\\hat{n_j} = \\sum_{i=1}^{n} p(j|i)$\n",
    "    \n",
    "    $\\hat{p_j} = \\frac{\\hat{n_j}}{n}$\n",
    "    \n",
    "    $\\hat{\\mu_j} = \\frac{1}{\\hat{n_j}} \\sum_{i = 1}^{n} \\hat{p_j} x_i$\n",
    "    \n",
    "    $\\hat{\\sigma_j^2} = \\frac{1}{\\hat{n_j}d} \\sum_{i=1}^{n} \\hat{p_j} \\cdot \\Vert x_i - \\hat{\\mu_j} \\Vert^2$\n",
    "    \n",
    "    \n",
    "4. Repeat iteratively until convergence\n",
    "\n",
    "The EM algorithm is only guaranteed to converge to a local minima. In reality, initialization in step (1) is typically applied with K-means to find initial cluster centers of $K$ clusters and use the global variance of the dataset as the initial variance of all the $K$ clusters.\n",
    "\n",
    "Generally, GMM can be extended to a mixture component with a general covariance matrix $\\Sigma_j$\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Question 1*\n",
    "\n",
    "Assume that the initial means and variances of two clusters in a GMM are as follows:\n",
    "\n",
    "$\\mu = \\begin{bmatrix} -3 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "$\\sigma_1^2 = \\sigma_2^2 = 4$\n",
    "\n",
    "$p_1 = p_2 = 0.5$\n",
    "\n",
    "$X = \\begin{bmatrix} 0.2 & -0.9 & -1 & 1.2 & 1.8 \\end{bmatrix}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T10:16:03.703676Z",
     "start_time": "2021-12-10T10:16:02.462151Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29421497216298875,\n",
       " 0.6224593312018545,\n",
       " 0.6513548646660543,\n",
       " 0.1066905939456512,\n",
       " 0.053403329799824234]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E-step: \n",
    "# Compute the posterior probabilities of each observation in cluster 1\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu = np.array([-3, 2])\n",
    "sigma = np.array([2, 2])\n",
    "p = np.array([0.5, 0.5])\n",
    "X = np.array([0.2, -0.9, -1, 1.2, 1.8])\n",
    "\n",
    "posterior1 = []\n",
    "\n",
    "for x in X:\n",
    "    m1, m2 = mu[0], mu[1]\n",
    "    sigma1, sigma2 = sigma[0], sigma[1]\n",
    "    p1, p2 = p[0], p[1]\n",
    "\n",
    "    l1 = norm.pdf(x, loc = m1, scale = sigma1)\n",
    "    l2 = norm.pdf(x, loc = m2, scale = sigma2)\n",
    "    denom = l1*p1+l2*p2\n",
    "    posterior = l1*p1/denom\n",
    "    posterior1.append(posterior)\n",
    "    \n",
    "posterior1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T10:16:03.713256Z",
     "start_time": "2021-12-10T10:16:03.708314Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.728123091776373 0.3456246183552746 -0.5373289474340417 0.5757859076870628\n"
     ]
    }
   ],
   "source": [
    "# M-step:\n",
    "# Compute the updated parameters corresponding to cluster 1\n",
    "\n",
    "n1_hat = sum(posterior1)\n",
    "p1_hat = n1_hat/X.shape[0]\n",
    "mu1_hat = 1/n1_hat * sum([posterior_i * x_i for posterior_i, x_i in zip(posterior1, X)])\n",
    "var_hat = 1/n1_hat * sum([posterior_i * (x_i - mu1_hat)**2 for posterior_i, x_i in zip(posterior1, X)])\n",
    "\n",
    "print(n1_hat, p1_hat, mu1_hat, var_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
