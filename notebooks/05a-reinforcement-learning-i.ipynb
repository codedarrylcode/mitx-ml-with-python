{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning I\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gentle intro to RL**\n",
    "\n",
    "RL algorithms learn to pick *good* actions based on the rewards that they receive during training, with none or limited supervision. \n",
    "\n",
    "The algorithm learns to take actions to maximize some notion of a *cumulative reward* instead of the immediate reward in the next step and can take *good* actions even without any intermediate rewards.\n",
    "\n",
    "Some terminology:\n",
    "\n",
    "- States, $s \\in S$ (observed)\n",
    "- Actions, $a \\in A$ (intended)\n",
    "- Transitions, $T(s, a, s') = p(s' | s, a)$\n",
    "    - A function that takes the current state, the intended action and outputs the probability of a specific next state, \n",
    "    - i.e. action dependent transition probabilities, such that for each state $s$ and action $a$, $\\sum_{s' \\in S} T(s, a, s') = 1$\n",
    "    \n",
    "    \n",
    "- Reward, $R(s, a, s')$, representing the reward for starting in state $s$, taking action $a$ (cost) and ending up in state $s'$ after one step\n",
    "\n",
    "These four values characterizes the *Markov Decision Process*:\n",
    "\n",
    "$MDP = \\text{<}S, A, T, R \\text{>}$\n",
    "\n",
    "****\n",
    "\n",
    "**Markov Decision Processes (MDP)**<br>\n",
    "\n",
    "MDPs satisfy the Markov property in that the transition probabilities and rewards depend only on the current state and action, and remain unchanged regardless of the history that leads to the current state.\n",
    "\n",
    "1. **Rewards**\n",
    "\n",
    "    One way to look at rewards is to define a bounded number of actions and states and aggregate all intermediate rewards, such that:\n",
    "\n",
    "    $\\text{Finite Horizon} = U([s_0, \\dots, s_{N+K}]) = U([s_0, \\dots, s_N]) \\forall K$\n",
    "\n",
    "    In this definition, the utility function, $U$, only looks at rewards up to $N$ steps and all other $K$ rewards past that point will be ignored. This definition can be problematic as it not only depends on the current state but also at the timepoint, i.e. if the agent is only left with one-step then it might take a highly risky move.\n",
    "\n",
    "    Consider a **discounted reward** utility function, that places higher value on the immediate step and value decays as rewards are further away, which allows us to look at an infinite horizon and does not depend on how many steps have been taken.\n",
    "\n",
    "    $U([s_0, \\dots]) = R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots = \\sum_{t=0}^{\\infty} \\gamma^t R(S_t)$\n",
    "\n",
    "    where \n",
    "\n",
    "    - $0 \\leq \\gamma \\leq 1$, such that for $\\gamma = 0$ then it boils down to greedily maximizing for the immediate reward\n",
    "    - $U([s_0, \\dots]) \\leq R_{max} \\sum_{t=0}^{\\infty} \\gamma^t = \\frac{R_{max}}{1 - \\gamma}$, i.e. if maximum reward is finite then $\\sum_{t=0}^{\\infty} \\gamma^t$ is a geometric series that converges to $\\frac{1}{1-\\gamma}$\n",
    "\n",
    "\n",
    "2. **Optimal Policy**\n",
    "\n",
    "    A policy is a function $\\pi : S \\rightarrow A$ that assigns an action $\\pi(s)$ to any state $s$ and we denote the optimal policy by $\\pi^*$ that maximizes the expected utility, even if it means taking actions that would lead to lower immediate next-step rewards from few states, i.e. this is exactly what MDP tries to solve.\n",
    "    \n",
    "    A value function $V^*(s)$ of a given state, $s$, is the expected reward (i.e. the expectation of the utility function) if the agent acts optimally starting at state $s$.\n",
    "    \n",
    "    A Q-function $Q^*(s,a)$ is the expected reward of a given state, $s$, and taking action $a$, and then acting optimally afterwards.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Bellman Equations**\n",
    "\n",
    "The Bellman equations connect $\\pi^*(s)$, $V^*(s)$ and $Q^*(s,a)$ together, such that:\n",
    "\n",
    "$V^*(s) = \\max_a Q^*(s, a) = Q^*(s, \\pi^*(s))$\n",
    "\n",
    "$Q^*(s,a) = \\sum_{s'} T(s, a, s') \\cdot [R(s,a,s') + \\gamma V^*(s')]$\n",
    "\n",
    "$\\therefore V^*(s) = \\max_a \\sum_{s'} T(s, a, s') \\cdot [R(s,a,s') + \\gamma V^*(s')]$\n",
    "\n",
    "where\n",
    "\n",
    "- $T(s, a, s')$ is the transition probability of being in state $s$, taking an action $a$ and entering a specific state $s'$\n",
    "- $R(s,a,s')$ is the reward for starting in state  ùë† , taking action  ùëé  (cost) and ending up in state  ùë†‚Ä≤  after one step\n",
    "\n",
    "These equations will help us to solve for the optimal MDP policy.\n",
    "\n",
    "****\n",
    "\n",
    "**Value Iteration Algorithm**\n",
    "\n",
    "Suppose we wish to look at the expected reward of the state after $K$ steps and is represented by $V^*_K (s)$ where as $K$ goes to $\\infty$ then $V^*_K \\rightarrow V(s)$\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "    $V^*_0 (s) = 0$\n",
    "\n",
    "\n",
    "2. Iterate and update until $V^*_K (s) \\simeq V^*_{K+1} (s)$ $\\forall s$\n",
    "\n",
    "    $V^*_{K+1} (s) = \\max_a \\sum_{s'} T(s, a, s') \\cdot [R(s,a,s') + \\gamma V^*_K(s')] $\n",
    "    \n",
    "\n",
    "Suppose we have an agent trying to navigate a one-dimensional grid consisting of 5 cells. At each step, the agent has only one action to choose from, i.e. it moves to the cell on the immediate right.\n",
    "\n",
    "<img alt=\"One-dimensional Grid\" src=\"assets/one_dimensional_grid.png\" width=\"500\">\n",
    "\n",
    "This example is such that the reward function is defined to be $R(s, a, s') = R(s)$, $R(s = 5) = 1$ or $R(s) = 0$ otherwise\n",
    "\n",
    "Let $V^*(i)$ denote the value function of state $i$, the $i^{th}$ cell starting from the left.\n",
    "\n",
    "Let $V^*_k (i)$ denote the value function estimate at state $i$ at the $k$th step of the value iteration algorithm. \n",
    "\n",
    "Let $V^*_0 (i)$ denote the initialization of this estimate.\n",
    "\n",
    "Suppose we use a discount factor $\\gamma = 0.5$, then we will write the functions $V^*_k$ as arrays below:\n",
    "\n",
    "$\\begin{bmatrix} V^*_k (1) & V^*_k (2) & V^*_k (3) & V^*_k (4) & V^*_k (5) \\end{bmatrix}$\n",
    "\n",
    "At step (1), we initialize by setting $V^*_0 (i) = 0$ for all $i$, such that:\n",
    "\n",
    "$V^*_0 = \\begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "Then, using the value iteration update rule, we get:\n",
    "\n",
    "If we walk 1 step further from a given state, then the value we will get is represented by $V^*_1 = \\begin{bmatrix} 0 & 0 & 0 & 0 & 1 \\end{bmatrix}$ where after 1 step we get $R(s) = 5$ for the fifth cell\n",
    "\n",
    "If we walk 2 steps further from a given state, then the value is represented as the following:\n",
    "\n",
    "$V^*_2 = \\begin{bmatrix} 0 & 0 & 0 & 0.5 & 1 \\end{bmatrix}$\n",
    "\n",
    "where $R(s,a,s') = R(4) = 0$ but $\\gamma V^*_2(5) = 0.5 \\times 1$\n",
    "\n",
    "Continue the iteraitons and update $V^*_K (s)$ until it converges\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*\n",
    "\n",
    "Consider the same one-dimensional grid, with changes to transition probabilities.\n",
    "\n",
    "<img alt=\"One-dimensional Grid\" src=\"assets/one_dimensional_grid.png\" width=\"300\">\n",
    "\n",
    "At any given grid location, the agent can choose to either stay at the location or move to an adjacent location. If the agent chooses to stay at the location, such an action is successful with probability 0.5 and\n",
    "\n",
    "- if the agent is at the corner grid locations then it ends up at the neighboring grid location with probability 0.5,\n",
    "- if the agent is at any of the inner grid locations it has a probability of 0.25 each of ending up at each of the neighboring locations\n",
    "\n",
    "If the agent chooses to move (*either left or right*) at any of the inner grid locations, such an action is successful with probability 1/3 and with probability 2/3 it fails to move, and\n",
    "\n",
    "- if the agent chooses to move left at the leftmost grid location, then the action ends up exactly the same as choosing to stay, i.e., staying at the leftmost grid location with probability 0.5, and ends up at its neighboring grid location with probability 0.5,\n",
    "- if the agent chooses to move right at the rightmost grid location, then the action ends up exactly the same as choosing to stay, i.e. staying at the rightmost grid location with probability 0.5, and ends up at its neighboring grid location with probability 0.5\n",
    "\n",
    "Let $\\gamma = 0.5$\n",
    "\n",
    "Run the value iteration algorithm for 10, 100 and 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent the transition probabilities and reward in a matrix\n",
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
