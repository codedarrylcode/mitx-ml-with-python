{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning II\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisiting Markov Decision Processes (MDP)**\n",
    "\n",
    "$\\text{Markov Decision Process, MDP} = \\text{<} S, A, T, R \\text{>}$\n",
    "\n",
    "where\n",
    "- $S$ refers to the set of all possible states\n",
    "- $A$ refers to the set of all possible actions\n",
    "- $T$ refers to the transition probabilities between states for a given action\n",
    "- $R$ refers to the reward expected, starting from state $s$, taking an action $a$ and ending up in the next state, $s'$\n",
    "\n",
    "The optimal policy, $\\pi^* (s)$, is the set of actions that would optimize the expected sum of rewards starting from state $s$. If an agent takes an action $a$ at state $s$ and then acts optimally after that, can be defined by a value function $Q^* (s,a) = \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma max_{a'} Q(s', a')]$\n",
    "\n",
    "These two equations are connected as $\\pi^* (s) = \\arg \\max_a Q^* (s,a)$, which is the optimal action taken at state $s$ and then acting optimally after that.\n",
    "\n",
    "****\n",
    "\n",
    "**Estimating reinforcement learning parameters**\n",
    "\n",
    "In RL, we may not know everything that is required in MDP. Realistically, $\\text{Reinforcement Learning, RL} = \\text{<} S, A {>}$ and we are likely to not know what are the transition probabilities between states and the rewards at each state. The high-level idea is to balance between **explore-exploit** when the apriori does not exist. Therefore the question is to *what extent do we explore, what do we explore and how it would change our recommendation for policies?*\n",
    "\n",
    "The empirical estimation of $T$, $R$ can be as follows:\n",
    "\n",
    "$\\hat{T}(s, a, s') = \\frac{\\text{count}(s, a, s')}{\\sum_{s'}\\text{count}(s, a, s')}$\n",
    "\n",
    "$\\hat{R}(s, a, s') = \\frac{\\sum_{t=1}^{\\text{count} (s, a, s')} R_t(s, a, s')}{\\text{count}(s, a, s')}$\n",
    "\n",
    "But realistic problems arise with such estimations as these statistics cannot be reliably collected unless the agent visits this state multiple times during the estimation process. This is especially problematic if the state space is large.\n",
    "\n",
    "****\n",
    "\n",
    "**Sampling-based approach for Q-learning**\n",
    "\n",
    "Suppose there is no information about $T$, $R$ and an agent starts out from state $s\n",
    "$ and collects a few samples by taking actions. Each sample will then collect information about the following tuple $((s, a, s'), R(s, a, s'))$ which indicates that the agent received a reward of $R(s, a, s')$ when it reached state $s'$ by taking action $a$ from state $s$.\n",
    "\n",
    "We can then collect $K$ samples:\n",
    "\n",
    "$sample_1 = R(s, a, s') + \\gamma max_{a'} Q(s', a')$\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "$sample_K = R(s, a, s_K') + \\gamma max_{a'} Q(s_K', a')$\n",
    "\n",
    "Here, our estimation* of $Q(s, a)$ can be done over $K$:\n",
    "\n",
    "$Q(s,a) = \\frac{1}{K} \\sum_{i=1}^{K} sample_i = \\frac{1}{K} \\sum_{i=1}^{K} [R(s, a, s_i') + \\gamma max_{a'} Q(s_i', a')]$\n",
    "\n",
    "$Q_{i+1} (s,a) = \\alpha \\cdot \\text{sample} + (1-\\alpha) \\cdot Q_i (s, a)$ using the exponential running average*\n",
    "\n",
    "Procedure to sampling-based Q-learning:\n",
    "\n",
    "1. Initialize $Q(s, a) = 0$ $\\forall s, a$\n",
    "2. Iterate until convergence\n",
    "    - Collect sample\n",
    "    - Update $Q_{i+1} = \\alpha \\cdot [R(s, a, s') + \\gamma max_{a'} Q_i (s', a')] + (1 - \\alpha) Q_i (s, a)$\n",
    "    - When re-written, it can be observed to be similar to a gradient descent algorithm:\n",
    "    \n",
    "        $Q_{i+1} = Q_i (s,a) + \\alpha [R(s, a, s') + \\gamma max_{a'} Q_i (s', a') - Q_i (s, a)]$\n",
    "    \n",
    "\n",
    "$\\therefore$ This algorithm guarantees convergence. Each time we take an action in the real world, we collect evidence, update our knowledge and recommend a set of actions based on what we know.\n",
    "\n",
    "In the question of exploration vs exploitation, one approach to balance both is the $\\epsilon$-greedy approach, which does this by randomly sampling an action with probability $\\epsilon$ and by choosing the best currently available option with probability $1 - \\epsilon$ such that the higher the $\\epsilon$ the higher are the chances that it explores new/less-visited states and actions. As the agent learns to act well and has sufficiently explored its environment, $\\epsilon$ should decay off.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Extras*\n",
    "\n",
    "Objective: Estimate $\\mathbb{E}[f(x)]$\n",
    "\n",
    "1. Model-based estimation of a probability distribution\n",
    "\n",
    "    - Sample $x_i \\sim p(X)$ for $i = 1, \\dots, K$\n",
    "    - $\\hat{p}(x) = \\frac{\\text{count}(x)}{K}$\n",
    "    - $\\mathbb{E}[f(x)] = \\sum_x p(x) \\cdot f(x)$\n",
    "\n",
    "2. Model-free, *does not estimate $\\hat{p}(x)$*\n",
    "\n",
    "    - Sample $x_i \\sim p(X)$ for $i = 1, \\dots, K$\n",
    "    - $\\mathbb{E}[f(x)] = \\frac{1}{K} \\sum_{i=1}^{K} f(x_i)$\n",
    "    \n",
    "Exponential running average\n",
    "\n",
    "$\\bar{X_n} = \\frac{X_n + (1 - \\alpha) X_{n-1} + (1 - \\alpha)^2 X_{n-2} + \\dots}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + \\dots}$\n",
    "\n",
    "$\\bar{X_n} = \\alpha X_n + (1 - \\alpha) \\bar{X}_{n-1}$\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
