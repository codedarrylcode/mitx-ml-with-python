{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gentle intro to RL**\n",
    "\n",
    "RL algorithms learn to pick *good* actions based on the rewards that they receive during training, with none or limited supervision. \n",
    "\n",
    "The algorithm learns to take actions to maximize some notion of a *cumulative reward* instead of the immediate reward in the next step and can take *good* actions even without any intermediate rewards.\n",
    "\n",
    "Some terminology:\n",
    "\n",
    "- States, $s \\in S$ (observed)\n",
    "- Actions, $a \\in A$ (intended)\n",
    "- Transitions, $T(s, a, s') = p(s' | s, a)$\n",
    "    - A function that takes the current state, the intended action and outputs the probability of a specific next state, \n",
    "    - i.e. action dependent transition probabilities, such that for each state $s$ and action $a$, $\\sum_{s' \\in S} T(s, a, s') = 1$\n",
    "    \n",
    "    \n",
    "- Reward, $R(s, a, s')$, representing the reward for starting in state $s$, taking action $a$ (cost) and ending up in state $s'$ after one step\n",
    "\n",
    "These four values characterizes the *Markov Decision Process*:\n",
    "\n",
    "$MDP = \\text{<}S, A, T, R \\text{>}$\n",
    "\n",
    "****\n",
    "\n",
    "**Markov Decision Processes (MDP)**<br>\n",
    "\n",
    "MDPs satisfy the Markov property in that the transition probabilities and rewards depend only on the current state and action, and remain unchanged regardless of the history that leads to the current state.\n",
    "\n",
    "1. **Rewards**\n",
    "\n",
    "    One way to look at rewards is to define a bounded number of actions and states and aggregate all intermediate rewards, such that:\n",
    "\n",
    "    $\\text{Finite Horizon} = U([s_0, \\dots, s_{N+K}]) = U([s_0, \\dots, s_N]) \\forall K$\n",
    "\n",
    "    In this definition, the utility function, $U$, only looks at rewards up to $N$ steps and all other $K$ rewards past that point will be ignored. This definition can be problematic as it not only depends on the current state but also at the timepoint, i.e. if the agent is only left with one-step then it might take a highly risky move.\n",
    "\n",
    "    Consider a **discounted reward** utility function, that places higher value on the immediate step and value decays as rewards are further away, which allows us to look at an infinite horizon and does not depend on how many steps have been taken.\n",
    "\n",
    "    $U([s_0, \\dots]) = R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\dots = \\sum_{t=0}^{\\infty} \\gamma^t R(S_t)$\n",
    "\n",
    "    where \n",
    "\n",
    "    - $0 \\leq \\gamma \\leq 1$, such that for $\\gamma = 0$ then it boils down to greedily maximizing for the immediate reward\n",
    "    - $U([s_0, \\dots]) \\leq R_{max} \\sum_{t=0}^{\\infty} \\gamma^t = \\frac{R_{max}}{1 - \\gamma}$, i.e. if maximum reward is finite then $\\sum_{t=0}^{\\infty} \\gamma^t$ is a geometric series that converges to $\\frac{1}{1-\\gamma}$\n",
    "\n",
    "\n",
    "2. **Optimal Policy**\n",
    "\n",
    "    A policy is a function $\\pi : S \\rightarrow A$ that assigns an action $\\pi(s)$ to any state $s$ and we denote the optimal policy by $\\pi^*$ that maximizes the expected utility, even if it means taking actions that would lead to lower immediate next-step rewards from few states, i.e. this is exactly what MDP tries to solve.\n",
    "    \n",
    "    A value function $V(s)$ of a given state, $s$, is the expected reward (i.e. the expectation of the utility function) if the agent acts optimally starting at state $s$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Bellman Equations**\n",
    "\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "**Value Iteration Algorithm**\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
